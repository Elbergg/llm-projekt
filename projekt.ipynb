{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIaoxJikBZyx"
      },
      "source": [
        "# sumaryzacja tekstu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoEGC0_4xnV0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFUtpmKDxnV1"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvRFkqxvHdNB"
      },
      "outputs": [],
      "source": [
        "!pip install \"datasets<3.0.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKjj9YaixnV3"
      },
      "outputs": [],
      "source": [
        "!pip install bert-extractive-summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWpCCK58xnV3"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vquItXi_RkR_"
      },
      "source": [
        "## importowanie datesetu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_Vp8NWm9UXC"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "ds = load_dataset(\"EdinburghNLP/xsum\", trust_remote_code=True)\n",
        "ds= ds.filter(lambda example: len(example[\"document\"]) < 1500)\n",
        "split_result = ds[\"test\"].train_test_split(test_size=2, seed=42)\n",
        "shots = split_result[\"test\"]\n",
        "ds[\"test\"] = split_result[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axmzUJ0zSqeg"
      },
      "outputs": [],
      "source": [
        "print(ds)\n",
        "print(shots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cbj5K1l2TfIo"
      },
      "outputs": [],
      "source": [
        "from summarizer import Summarizer\n",
        "model_bertsum = Summarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWb8kiPLTzL4"
      },
      "outputs": [],
      "source": [
        "from ignite.metrics import Rouge\n",
        "\n",
        "m = Rouge(variants=[\"L\", 1], multiref=\"best\")\n",
        "\n",
        "candidate = \"the cat is not there\".split()\n",
        "references = [\n",
        "    \"the cat is on the mat\".split(),\n",
        "    \"there is a cat on the mat\".split()\n",
        "]\n",
        "\n",
        "m.update(([candidate], [references]))\n",
        "\n",
        "print(m.compute())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PB2Lv4ZUhqaU"
      },
      "outputs": [],
      "source": [
        "num_workers = 1\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL1QHhighdfi"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "test_dataset = ds[\"test\"].select(range(50))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "print(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaBqwH-einbT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    if not text:\n",
        "        return [\".\"]\n",
        "    text = text.lower().translate(\n",
        "        str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
        "    )\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def rouge_score_loss(\n",
        "    batch,\n",
        "    model,\n",
        "    bert: bool,\n",
        "    tokenizer: any,\n",
        "    prompt=None,\n",
        "    promptsuff=None,\n",
        "    m=Rouge(variants=[\"L\", 2], multiref=\"best\"),\n",
        "    prints = False,\n",
        "    chat = False\n",
        "):\n",
        "    if bert:\n",
        "        predictions = [\n",
        "            model(doc, ratio=0.1) for doc in batch[\"document\"]\n",
        "        ]\n",
        "    else:\n",
        "        docs = [doc for doc in batch[\"document\"]]\n",
        "\n",
        "        prompted = [\n",
        "            f\"{prompt}{doc}{promptsuff}\" for doc in batch[\"document\"]\n",
        "        ]\n",
        "        if chat:\n",
        "          prompted = [\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "          ]\n",
        "\n",
        "          text = tokenizer.apply_chat_template(\n",
        "              prompted,\n",
        "              tokenize=False,\n",
        "              add_generation_prompt=True\n",
        "          )\n",
        "          inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "        else:\n",
        "          inputs = tokenizer(\n",
        "              prompted,\n",
        "              return_tensors=\"pt\",\n",
        "              padding=True,\n",
        "              truncation=True,\n",
        "              max_length=4096,\n",
        "          )\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            top_k=0,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        new_tokens = summary_ids[:, input_length:]\n",
        "\n",
        "        predictions = tokenizer.batch_decode(\n",
        "            new_tokens, skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    if prints:\n",
        "      # print(f\"Doc: {batch['document'][0]}\")\n",
        "      print(f\"Prediction: {predictions[0]}\")\n",
        "      # print(f\"Target: {batch['summary'][0]}\")\n",
        "\n",
        "    predictions = [\n",
        "        clean(pred) if pred.strip() else \".\" for pred in predictions\n",
        "    ]\n",
        "\n",
        "    targets = [[clean(ref)] for ref in batch[\"summary\"]]\n",
        "    m.update((predictions, targets))\n",
        "    # print(m.compute())\n",
        "    return m.compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_data(data, labels=['Rouge-1-P', 'Rouge-1-R', 'Rouge-1-F','Rouge-2-P', 'Rouge-2-R', 'Rouge-2-F',  'Rouge-L-P', 'Rouge-L-R', 'Rouge-L-F', ], models = ['Qwen 1.5 0.5B - LoRa', 'Qwen 1.5 0.5B - few shot 1', 'Qwen 1.5 0.5B - few shot 3', 'Qwen 1.5 0.5B - zero shot', 'Bertsum - bert-base-uncased', 'Bertsum - Sbertmodel', 'Bertsum - roberta-base','Qwen 2.5 3B - zero shot', 'Qwen 2.5 3B - few shot 1',  'Qwen 2.5 3B - few shot 3']):\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.075\n",
        "\n",
        "    plt.figure(figsize=(40, 12))\n",
        "    for i, row in enumerate(data):\n",
        "        offset = x + (i * width)\n",
        "        plt.bar(offset, row, width=width, label=models[i])\n",
        "    center_offset = ((len(data) - 1) * width) / 2\n",
        "    plt.xticks(x + center_offset, labels)\n",
        "\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('ROUGE Score Comparison')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7ALm8sXk5vPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vmrcl6FcL6j"
      },
      "outputs": [],
      "source": [
        "def batch_to_device(batch: dict, device) -> dict:\n",
        "    return {k: v.to(device) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBlLn019iPyA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "def calc_loss_batch(batch: dict, model: nn.Module) -> Tensor:\n",
        "    logits = model(batch['input_ids'])\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), batch['labels'].flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSq_WPlziO6G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "def calc_loss_loader(\n",
        "    data_loader,\n",
        "    model,\n",
        "    device,\n",
        "    bert: bool,\n",
        "    tokenizer: any,\n",
        "    prompt=None,\n",
        "    promptsuff=None,\n",
        "    prints = False,\n",
        "    chat=False\n",
        ") -> float:\n",
        "    results = []\n",
        "    length = len(data_loader)\n",
        "    for i, batch in enumerate(data_loader):\n",
        "        m = Rouge(variants=[1,2,\"L\"], multiref=\"best\")\n",
        "        loader_bar = tqdm(data_loader, desc=\"Calculating ROUGE average score\", leave=False)\n",
        "        ret = rouge_score_loss(\n",
        "            batch,\n",
        "            model,\n",
        "            bert,\n",
        "            tokenizer,\n",
        "            prompt,\n",
        "            promptsuff,\n",
        "            m,\n",
        "            prints,\n",
        "            chat,\n",
        "        )\n",
        "        results.append(list(ret.values()))\n",
        "\n",
        "    results = np.array(results)\n",
        "    return np.mean(results, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a39GKm2iLIN"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device,\n",
        "    bert: bool,\n",
        "    tokenizer: any,\n",
        "    prompt: str = \"\",\n",
        "    promptsuff: str = \"\",\n",
        "    prints = False,\n",
        "    chat = False\n",
        ") -> tuple[float, float]:\n",
        "    if not bert:\n",
        "        model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss = calc_loss_loader(\n",
        "            loader,\n",
        "            model,\n",
        "            device,\n",
        "            bert=bert,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            promptsuff=promptsuff,\n",
        "            prints=prints,\n",
        "            chat=chat\n",
        "        )\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iBrsae0g1Ba"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "data_bertsum = evaluate_model(model_bertsum, test_loader, device='cuda', bert=True, tokenizer=None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer.sbert import SBertSummarizer\n",
        "model_sbert = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
        "data_bertsum_sbert = evaluate_model(model_sbert, test_loader, device='cuda', bert=True, tokenizer=None)"
      ],
      "metadata": {
        "id": "s6Fw92PB4RgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import TransformerSummarizer\n",
        "model_roberta = TransformerSummarizer(transformer_type=\"Roberta\", transformer_model_key=\"roberta-base\")\n",
        "data_bertsum_roberta = evaluate_model(model_roberta, test_loader, device='cuda', bert=True, tokenizer=None)"
      ],
      "metadata": {
        "id": "H3PUKkjf_rA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8UF34yIrqcQ"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(tokenizer, size):\n",
        "    prompt = \"\"\n",
        "    shots = ds[\"train\"].train_test_split(train_size=size, seed=35)[\"train\"]\n",
        "    shots_loader = DataLoader(shots, batch_size=100000, shuffle=True, num_workers=num_workers)\n",
        "    for batch in shots_loader:\n",
        "        for i in range(len(batch['document'])):\n",
        "            prompt += \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.\"\n",
        "            prompt += f'[TEXT]{batch['document'][i]}[AGENT]{batch['summary'][i]}'\n",
        "            prompt += tokenizer.eos_token + \"\\n\"\n",
        "    prompt += \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.[TEXT]\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y unsloth unsloth_zoo\n",
        "\n",
        "# 2. Install Unsloth Zoo DIRECTLY from Git (to get 'tiled_mlp')\n",
        "!pip install --upgrade --no-cache-dir \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "# 3. Install Unsloth main library from Git\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "PM0XE6C-QGdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3tgSeAEdQr3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import FastLanguageModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "try:\n",
        "  del model\n",
        "  del tokenizer3\n",
        "  del model\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "l7f4AuVzSSvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ni9Iw1ofzKy"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen1.5-0.5B\", max_seq_length=32000, load_in_4bit=False\n",
        ")\n",
        "prompt1 = generate_prompt(tokenizer, 1)\n",
        "print(prompt1)\n",
        "torch.cuda.empty_cache()\n",
        "data_1 = evaluate_model(model, test_loader, device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt1, promptsuff = \"[AGENT]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt3 = generate_prompt(tokenizer, 3)\n",
        "torch.cuda.empty_cache()\n",
        "data_3 = evaluate_model(model, test_loader, device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt3, promptsuff = \"[AGENT]\")"
      ],
      "metadata": {
        "id": "LBqLk5RUkPje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1Pi4BAxnV8"
      },
      "outputs": [],
      "source": [
        "prompt0 = \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.[TEXT]\"\n",
        "data_zero_shot = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        device=\"cuda\",\n",
        "        bert=False,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt0,\n",
        "        promptsuff = \"[AGENT]\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wil7940urFG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "model_path = './drive/MyDrive/models/ft'\n",
        "ft_model = PeftModel.from_pretrained(model, model_path, max_sequence_length=32000, load_in_4bit=False)\n"
      ],
      "metadata": {
        "id": "86iKOMedrXdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjZzmYmmxnV-"
      },
      "outputs": [],
      "source": [
        "data_ft = evaluate_model(ft_model, test_loader, 'cuda', bert=False, tokenizer = tokenizer, prompt = \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.[TEXT]\", promptsuff = \"[AGENT]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_instruct = \"Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.\"\n",
        "model15_instruct, tokenizer_instruct_15 = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen1.5-0.5B-chat\", max_seq_length=32000, load_in_4bit=False\n",
        ")\n",
        "data_instruct_15 = evaluate_model(model15_instruct, test_loader, device='cuda', bert=False, tokenizer=tokenizer_instruct_15, prompt=prompt_instruct, promptsuff = \"[AGENT]\", chat=True)"
      ],
      "metadata": {
        "id": "Qw1NExPySP0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model25_instruct, tokenizer_instruct_25 = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-0.5B-instruct\", max_seq_length=32000, load_in_4bit=False\n",
        ")\n",
        "data_instruct_25 = evaluate_model(model15_instruct, test_loader, device='cuda', bert=False, tokenizer=tokenizer_instruct_25, prompt=prompt_instruct, promptsuff = \"[AGENT]\", chat=True)"
      ],
      "metadata": {
        "id": "VNxuqncPTDCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_better, tokenizer_better = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-3B\", max_seq_length=32000, load_in_4bit=False\n",
        ")\n",
        "torch.cuda.empty_cache()\n",
        "data_better = evaluate_model(model_better, test_loader, device='cuda', bert=False, tokenizer=tokenizer_better, prompt=prompt0, promptsuff = \"[AGENT]\")"
      ],
      "metadata": {
        "id": "b2cQ-HMkkulA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1_better = generate_prompt(tokenizer_better, 1)\n",
        "data_better_1 = evaluate_model(model_better, test_loader, device='cuda', bert=False, tokenizer=tokenizer_better, prompt=prompt1_better, promptsuff = \"[AGENT]\")"
      ],
      "metadata": {
        "id": "safNS3Es_3fK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3_better = generate_prompt(tokenizer_better, 3)\n",
        "data_better_3 = evaluate_model(model_better, test_loader, device='cuda', bert=False, tokenizer=tokenizer_better, prompt=prompt3_better, promptsuff = \"[AGENT]\")"
      ],
      "metadata": {
        "id": "eaNI0n0c_5ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data([data_ft, data_1,data_3, data_zero_shot,data_instruct_15, data_bertsum, data_bertsum_sbert,data_bertsum_roberta, data_better, data_better_1, data_better_3, data_instruct_25])\n"
      ],
      "metadata": {
        "id": "RgrAlrnonkfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_index = random.randint(0, len(ds[\"test\"]) - 1)\n",
        "showcase_dataset = ds[\"test\"].select([random_index])\n",
        "showcase_loader = DataLoader(showcase_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "batch = next(iter(showcase_loader))\n",
        "print(f\"Document: {batch['document'][0]}\")\n",
        "print(f\"Target: {batch['summary'][0]}\")\n",
        "print(\"Qwuen 1.5 0.5B zero shot:\")\n",
        "evaluate_model(model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt0, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B one shot:\")\n",
        "evaluate_model(model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt1, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B three shot:\")\n",
        "evaluate_model(model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt3, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B fine tuned:\")\n",
        "evaluate_model(ft_model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt0, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B chat:\")\n",
        "evaluate_model(model15_instruct, test_loader, device='cuda', bert=False, tokenizer=tokenizer_instruct_15, prompt=prompt_instruct, chat=True)\n",
        "print(\"Qwuen 2.5 0.5B instruct:\")\n",
        "evaluate_model(model25_instruct, test_loader, device='cuda', bert=False, tokenizer=tokenizer_instruct_25, prompt=prompt_instruct, chat=True)\n",
        "print(\"Qwuen 2.5 3B zero shot:\")\n",
        "evaluate_model(model_better, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer_better, prompt=prompt0, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 2.5 3B one shot:\")\n",
        "evaluate_model(model_better, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer_better, prompt=prompt1_better, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 2.5 3B three shot:\")\n",
        "evaluate_model(model_better, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer_better, prompt=prompt3_better, promptsuff = \"[AGENT]\", prints = True)\n",
        "print('Bertsum - bert-base-uncased')\n",
        "evaluate_model(model_bertsum, showcase_loader, device='cuda', bert=True, tokenizer=None, prints = True)\n",
        "print('Bertsum - Sbertmodel')\n",
        "evaluate_model(model_sbert, showcase_loader, device='cuda', bert=True, tokenizer=None, prints = True)\n",
        "print('Bertsum - roberta-base')\n",
        "evaluate_model(model_roberta, showcase_loader, device='cuda', bert=True, tokenizer=None, prints = True)"
      ],
      "metadata": {
        "id": "H24v7knlprYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_qwen15 = np.array([data_zero_shot, data_1, data_3, data_ft])\n",
        "data_bertsums = np.array([data_bertsum, data_bertsum_sbert, data_bertsum_roberta])\n",
        "data_qwen25 = np.array([data_better, data_better_1, data_better_3])\n",
        "\n",
        "print(data_qwen15)"
      ],
      "metadata": {
        "id": "dF372XbzEQ4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_bertsums)"
      ],
      "metadata": {
        "id": "PGGleruOEmHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_qwen25)"
      ],
      "metadata": {
        "id": "SpNqZBq5EnjB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}