{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIaoxJikBZyx"
      },
      "source": [
        "# sumaryzacja tekstu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoEGC0_4xnV0",
        "outputId": "4d8de525-ab16-4566-cbd3-abfc7f230e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFUtpmKDxnV1",
        "outputId": "e70043dc-66c3-4d93-f32b-d4e4705a1f38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jan 31 16:44:20 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8             17W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvRFkqxvHdNB",
        "outputId": "2843a67f-01cb-4df3-a7b2-f87fef4cd416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets<3.0.0\n",
            "  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (3.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (23.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets<3.0.0) (6.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets<3.0.0) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets<3.0.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.21.2->datasets<3.0.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<3.0.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<3.0.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<3.0.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets<3.0.0) (2026.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3.0.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3.0.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets<3.0.0) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets<3.0.0) (1.17.0)\n",
            "Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.3.0\n",
            "    Uninstalling datasets-4.3.0:\n",
            "      Successfully uninstalled datasets-4.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "unsloth-zoo 2026.1.4 requires datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1, but you have datasets 2.21.0 which is incompatible.\n",
            "trl 0.24.0 requires datasets>=3.0.0, but you have datasets 2.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"datasets<3.0.0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKjj9YaixnV3",
        "outputId": "93b50387-8efe-45d7-be09-86ca10a472a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.12/dist-packages (0.10.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from bert-extractive-summarizer) (4.57.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from bert-extractive-summarizer) (1.6.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (from bert-extractive-summarizer) (3.8.11)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->bert-extractive-summarizer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy->bert-extractive-summarizer) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers->bert-extractive-summarizer) (3.20.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->bert-extractive-summarizer) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->bert-extractive-summarizer) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->bert-extractive-summarizer) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->bert-extractive-summarizer) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->bert-extractive-summarizer) (0.7.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->bert-extractive-summarizer) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->bert-extractive-summarizer) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->bert-extractive-summarizer) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy->bert-extractive-summarizer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->bert-extractive-summarizer) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy->bert-extractive-summarizer) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy->bert-extractive-summarizer) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-extractive-summarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWpCCK58xnV3",
        "outputId": "d775c536-1d47-4071-bffb-770857178f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.12/dist-packages (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pytorch-ignite) (25.0)\n",
            "Requirement already satisfied: torch<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from pytorch-ignite) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=1.10->pytorch-ignite) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=1.10->pytorch-ignite) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=1.10->pytorch-ignite) (3.0.3)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.12/dist-packages (1.8.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.9.0+cu126)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.3)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vquItXi_RkR_"
      },
      "source": [
        "## importowanie datesetu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "D_Vp8NWm9UXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a620364-bf15-4324-a20c-386427d8ae79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "ds = load_dataset(\"EdinburghNLP/xsum\", trust_remote_code=True)\n",
        "ds= ds.filter(lambda example: len(example[\"document\"]) < 1500)\n",
        "split_result = ds[\"test\"].train_test_split(test_size=2, seed=42)\n",
        "shots = split_result[\"test\"]\n",
        "ds[\"test\"] = split_result[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axmzUJ0zSqeg",
        "outputId": "a6b7678d-051f-41b3-c174-5f0860465ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['document', 'summary', 'id'],\n",
            "        num_rows: 84554\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['document', 'summary', 'id'],\n",
            "        num_rows: 4705\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['document', 'summary', 'id'],\n",
            "        num_rows: 4689\n",
            "    })\n",
            "})\n",
            "Dataset({\n",
            "    features: ['document', 'summary', 'id'],\n",
            "    num_rows: 2\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(ds)\n",
        "print(shots)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbj5K1l2TfIo",
        "outputId": "58a72b5e-c749-438e-d854-264a0cfcf163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao:Skipping import of cpp extensions due to incompatible torch version 2.9.0+cu126 for torchao version 0.15.0             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        }
      ],
      "source": [
        "from summarizer import Summarizer\n",
        "model_bertsum = Summarizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWb8kiPLTzL4",
        "outputId": "88f5a3e8-821b-410f-89bf-abc5804705a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Rouge-L-P': 0.6, 'Rouge-L-R': 0.5, 'Rouge-L-F': 0.5, 'Rouge-1-P': 0.8, 'Rouge-1-R': 0.5714285714285714, 'Rouge-1-F': 0.5714285714285714}\n"
          ]
        }
      ],
      "source": [
        "from ignite.metrics import Rouge\n",
        "\n",
        "m = Rouge(variants=[\"L\", 1], multiref=\"best\")\n",
        "\n",
        "candidate = \"the cat is not there\".split()\n",
        "references = [\n",
        "    \"the cat is on the mat\".split(),\n",
        "    \"there is a cat on the mat\".split()\n",
        "]\n",
        "\n",
        "m.update(([candidate], [references]))\n",
        "\n",
        "print(m.compute())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PB2Lv4ZUhqaU"
      },
      "outputs": [],
      "source": [
        "num_workers = 1\n",
        "batch_size = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "eL1QHhighdfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71fcfc9c-90db-401e-b5c0-83757e220df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['document', 'summary', 'id'],\n",
            "    num_rows: 50\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "test_dataset = ds[\"test\"].select(range(50))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "print(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WaBqwH-einbT"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    if not text:\n",
        "        return [\".\"]\n",
        "    text = text.lower().translate(\n",
        "        str.maketrans(string.punctuation, \" \" * len(string.punctuation))\n",
        "    )\n",
        "    return text.split()\n",
        "\n",
        "\n",
        "def rouge_score_loss(\n",
        "    batch,\n",
        "    model,\n",
        "    bert: bool,\n",
        "    tokenizer: any,\n",
        "    prompt=None,\n",
        "    promptsuff=None,\n",
        "    m=Rouge(variants=[\"L\", 2], multiref=\"best\"),\n",
        "    prints = False,\n",
        "):\n",
        "    if bert:\n",
        "        predictions = [\n",
        "            model(doc, ratio=0.1) for doc in batch[\"document\"]\n",
        "        ]\n",
        "    else:\n",
        "        docs = [doc for doc in batch[\"document\"]]\n",
        "\n",
        "        prompted = [\n",
        "            f\"{prompt}{doc}{promptsuff}\" for doc in batch[\"document\"]\n",
        "        ]\n",
        "        inputs = tokenizer(\n",
        "            prompted,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=4096,\n",
        "        )\n",
        "\n",
        "        device = next(model.parameters()).device\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        summary_ids = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            top_k=0,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "        new_tokens = summary_ids[:, input_length:]\n",
        "\n",
        "        predictions = tokenizer.batch_decode(\n",
        "            new_tokens, skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    if prints:\n",
        "      # print(f\"Doc: {batch['document'][0]}\")\n",
        "      print(f\"Prediction: {predictions[0]}\")\n",
        "      # print(f\"Target: {batch['summary'][0]}\")\n",
        "\n",
        "    predictions = [\n",
        "        clean(pred) if pred.strip() else \".\" for pred in predictions\n",
        "    ]\n",
        "\n",
        "    targets = [[clean(ref)] for ref in batch[\"summary\"]]\n",
        "    m.update((predictions, targets))\n",
        "    # print(m.compute())\n",
        "    return m.compute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_data(data, labels=['Rouge-1-P', 'Rouge-1-R', 'Rouge-1-F','Rouge-2-P', 'Rouge-2-R', 'Rouge-2-F',  'Rouge-L-P', 'Rouge-L-R', 'Rouge-L-F', ], models = ['Qwen 1.5 0.5B - LoRa', 'Qwen 1.5 0.5B - few shot 2', 'Qwen 1.5 0.5B - few shot 4', 'Qwen 1.5 0.5B - zero shot', 'Bertsum - bert-base-uncased', 'Bertsum - Sbertmodel','Qwen 2.5 3B - zero shot']):\n",
        "    x = np.arange(len(labels))\n",
        "    width = 0.1\n",
        "\n",
        "    plt.figure(figsize=(25, 6))\n",
        "    for i, row in enumerate(data):\n",
        "        offset = x + (i * width)\n",
        "        plt.bar(offset, row, width=width, label=models[i])\n",
        "    center_offset = ((len(data) - 1) * width) / 2\n",
        "    plt.xticks(x + center_offset, labels)\n",
        "\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel('Score')\n",
        "    plt.title('ROUGE Score Comparison')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7ALm8sXk5vPW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9vmrcl6FcL6j"
      },
      "outputs": [],
      "source": [
        "def batch_to_device(batch: dict, device) -> dict:\n",
        "    return {k: v.to(device) for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VBlLn019iPyA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "def calc_loss_batch(batch: dict, model: nn.Module) -> Tensor:\n",
        "    logits = model(batch['input_ids'])\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), batch['labels'].flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LSq_WPlziO6G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def calc_loss_loader(\n",
        "    data_loader,\n",
        "    model,\n",
        "    device,\n",
        "    bert: bool,\n",
        "    tokenizer: any,\n",
        "    prompt=None,\n",
        "    promptsuff=None,\n",
        "    prints = False,\n",
        ") -> float:\n",
        "    results = []\n",
        "    print(f\"{len(data_loader)} batches\")\n",
        "    for i, batch in enumerate(data_loader):\n",
        "\n",
        "        m = Rouge(variants=[1,2,\"L\"], multiref=\"best\")\n",
        "        print(f\"Batch: {i+1}\")\n",
        "        ret = rouge_score_loss(\n",
        "            batch,\n",
        "            model,\n",
        "            bert,\n",
        "            tokenizer,\n",
        "            prompt,\n",
        "            promptsuff,\n",
        "            m,\n",
        "            prints\n",
        "        )\n",
        "        results.append(list(ret.values()))\n",
        "\n",
        "    results = np.array(results)\n",
        "    return np.mean(results, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0a39GKm2iLIN"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device,\n",
        "    bert: bool,\n",
        "    tokenizer: any,\n",
        "    prompt: str = \"\",\n",
        "    promptsuff: str = \"\",\n",
        "    prints = False\n",
        ") -> tuple[float, float]:\n",
        "    if not bert:\n",
        "        model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss = calc_loss_loader(\n",
        "            loader,\n",
        "            model,\n",
        "            device,\n",
        "            bert=bert,\n",
        "            tokenizer=tokenizer,\n",
        "            prompt=prompt,\n",
        "            promptsuff=promptsuff,\n",
        "            prints=prints,\n",
        "        )\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8iBrsae0g1Ba",
        "outputId": "b1e14d43-2655-4541-b079-9b70741d023b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 batches\n",
            "Batch: 1\n",
            "Batch: 2\n",
            "Batch: 3\n",
            "Batch: 4\n",
            "Batch: 5\n",
            "Batch: 6\n",
            "Batch: 7\n",
            "Batch: 8\n",
            "Batch: 9\n",
            "Batch: 10\n",
            "Batch: 11\n",
            "Batch: 12\n",
            "Batch: 13\n",
            "Batch: 14\n",
            "Batch: 15\n",
            "Batch: 16\n",
            "Batch: 17\n",
            "Batch: 18\n",
            "Batch: 19\n",
            "Batch: 20\n",
            "Batch: 21\n",
            "Batch: 22\n",
            "Batch: 23\n",
            "Batch: 24\n",
            "Batch: 25\n",
            "Batch: 26\n",
            "Batch: 27\n",
            "Batch: 28\n",
            "Batch: 29\n",
            "Batch: 30\n",
            "Batch: 31\n",
            "Batch: 32\n",
            "Batch: 33\n",
            "Batch: 34\n",
            "Batch: 35\n",
            "Batch: 36\n",
            "Batch: 37\n",
            "Batch: 38\n",
            "Batch: 39\n",
            "Batch: 40\n",
            "Batch: 41\n",
            "Batch: 42\n",
            "Batch: 43\n",
            "Batch: 44\n",
            "Batch: 45\n",
            "Batch: 46\n",
            "Batch: 47\n",
            "Batch: 48\n",
            "Batch: 49\n",
            "Batch: 50\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "data_bertsum = evaluate_model(model_bertsum, test_loader, device='cuda', bert=True, tokenizer=None)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer.sbert import SBertSummarizer\n",
        "model_sbert = SBertSummarizer('paraphrase-MiniLM-L6-v2')\n",
        "data_bertsum_sbert = evaluate_model(model_sbert, test_loader, device='cuda', bert=True, tokenizer=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Fw92PB4RgF",
        "outputId": "842c08df-2f05-48bc-d820-bb727cef763a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 batches\n",
            "Batch: 1\n",
            "Batch: 2\n",
            "Batch: 3\n",
            "Batch: 4\n",
            "Batch: 5\n",
            "Batch: 6\n",
            "Batch: 7\n",
            "Batch: 8\n",
            "Batch: 9\n",
            "Batch: 10\n",
            "Batch: 11\n",
            "Batch: 12\n",
            "Batch: 13\n",
            "Batch: 14\n",
            "Batch: 15\n",
            "Batch: 16\n",
            "Batch: 17\n",
            "Batch: 18\n",
            "Batch: 19\n",
            "Batch: 20\n",
            "Batch: 21\n",
            "Batch: 22\n",
            "Batch: 23\n",
            "Batch: 24\n",
            "Batch: 25\n",
            "Batch: 26\n",
            "Batch: 27\n",
            "Batch: 28\n",
            "Batch: 29\n",
            "Batch: 30\n",
            "Batch: 31\n",
            "Batch: 32\n",
            "Batch: 33\n",
            "Batch: 34\n",
            "Batch: 35\n",
            "Batch: 36\n",
            "Batch: 37\n",
            "Batch: 38\n",
            "Batch: 39\n",
            "Batch: 40\n",
            "Batch: 41\n",
            "Batch: 42\n",
            "Batch: 43\n",
            "Batch: 44\n",
            "Batch: 45\n",
            "Batch: 46\n",
            "Batch: 47\n",
            "Batch: 48\n",
            "Batch: 49\n",
            "Batch: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "h8UF34yIrqcQ"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(tokenizer, size):\n",
        "    prompt = \"\"\n",
        "    shots = ds[\"train\"].train_test_split(train_size=4, seed=35)[\"train\"]\n",
        "    shots_loader = DataLoader(shots, batch_size=100000, shuffle=True, num_workers=num_workers)\n",
        "    for batch in shots_loader:\n",
        "        for i in range(len(batch['document'])):\n",
        "            prompt += \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.\"\n",
        "            prompt += f'[TEXT]{batch['document'][i]}[AGENT]{batch['summary'][i]}'\n",
        "            prompt += tokenizer.eos_token + \"\\n\"\n",
        "    prompt += \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.[TEXT]\"\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y unsloth unsloth_zoo\n",
        "\n",
        "# 2. Install Unsloth Zoo DIRECTLY from Git (to get 'tiled_mlp')\n",
        "!pip install --upgrade --no-cache-dir \"unsloth_zoo @ git+https://github.com/unslothai/unsloth-zoo.git\"\n",
        "\n",
        "# 3. Install Unsloth main library from Git\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM0XE6C-QGdh",
        "outputId": "d4572b8a-474b-446e-9348-292ec9640e5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: unsloth 2026.1.4\n",
            "Uninstalling unsloth-2026.1.4:\n",
            "  Successfully uninstalled unsloth-2026.1.4\n",
            "Found existing installation: unsloth_zoo 2026.1.4\n",
            "Uninstalling unsloth_zoo-2026.1.4:\n",
            "  Successfully uninstalled unsloth_zoo-2026.1.4\n",
            "Collecting unsloth_zoo@ git+https://github.com/unslothai/unsloth-zoo.git\n",
            "  Cloning https://github.com/unslothai/unsloth-zoo.git to /tmp/pip-install-80ht9gim/unsloth-zoo_3ee95a30d60244dab41915fcb84ed95c\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth-zoo.git /tmp/pip-install-80ht9gim/unsloth-zoo_3ee95a30d60244dab41915fcb84ed95c\n",
            "  Resolved https://github.com/unslothai/unsloth-zoo.git to commit 2a80d543b9e22f68e051e32029c8a47005102895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3tgSeAEdQr3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from unsloth import FastLanguageModel\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "try:\n",
        "  del model\n",
        "  del tokenizer3\n",
        "  del model\n",
        "except NameError:\n",
        "  pass\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "l7f4AuVzSSvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ni9Iw1ofzKy"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen1.5-0.5B\", max_seq_length=32000, load_in_4bit=False\n",
        ")\n",
        "prompt2 = generate_prompt(tokenizer, 2)\n",
        "print(prompt2)\n",
        "torch.cuda.empty_cache()\n",
        "data_2 = evaluate_model(model, test_loader, device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt2, promptsuff = \"[AGENT]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prompt4 = generate_prompt(tokenizer, 4)\n",
        "torch.cuda.empty_cache()\n",
        "data_4 = evaluate_model(model, test_loader, device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt4, promptsuff = \"[AGENT]\")"
      ],
      "metadata": {
        "id": "LBqLk5RUkPje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0O1Pi4BAxnV8"
      },
      "outputs": [],
      "source": [
        "prompt0 = \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.[TEXT]\"\n",
        "data_zero_shot = evaluate_model(\n",
        "        model,\n",
        "        test_loader,\n",
        "        device=\"cuda\",\n",
        "        bert=False,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt=prompt0,\n",
        "        promptsuff = \"[AGENT]\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wil7940urFG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "model_path = './drive/MyDrive/models/ft'\n",
        "ft_model = PeftModel.from_pretrained(model, model_path, max_sequence_length=32000, load_in_4bit=False)\n"
      ],
      "metadata": {
        "id": "86iKOMedrXdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjZzmYmmxnV-"
      },
      "outputs": [],
      "source": [
        "data_ft = evaluate_model(ft_model, test_loader, 'cuda', bert=False, tokenizer = tokenizer, prompt = \"[USER] Summarize this text. Don't hallucinate. Be thorough and explicit. Always provide an anwer.[TEXT]\", promptsuff = \"[AGENT]\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_better, tokenizer_better = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Qwen/Qwen2.5-3B\", max_seq_length=32000, load_in_4bit=False\n",
        ")\n",
        "print(prompt0)\n",
        "torch.cuda.empty_cache()\n",
        "data_better = evaluate_model(model_better, test_loader, device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt0, promptsuff = \"[AGENT]\")"
      ],
      "metadata": {
        "id": "b2cQ-HMkkulA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data([data_ft, data_2,data_4, data_zero_shot, data_bertsum, data_bertsum_sbert, data_better])\n"
      ],
      "metadata": {
        "id": "RgrAlrnonkfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random_index = random.randint(0, len(ds[\"test\"]) - 1)\n",
        "showcase_dataset = ds[\"test\"].select([0])\n",
        "showcase_loader = DataLoader(showcase_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "batch = next(iter(showcase_loader))\n",
        "print(f\"Document: {batch['document'][0]}\")\n",
        "print(f\"Target: {batch['summary'][0]}\")\n",
        "print(\"Qwuen 1.5 0.5B zero shot:\")\n",
        "evaluate_model(model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt0, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B two shot:\")\n",
        "evaluate_model(model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt2, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B four shot:\")\n",
        "evaluate_model(model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt4, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 1.5 0.5B fine tuned:\")\n",
        "evaluate_model(ft_model, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt0, promptsuff = \"[AGENT]\", prints = True)\n",
        "print(\"Qwuen 2.5 3B zero shot:\")\n",
        "evaluate_model(model_better, showcase_loader , device='cuda', bert=False, tokenizer=tokenizer, prompt=prompt0, promptsuff = \"[AGENT]\", prints = True)"
      ],
      "metadata": {
        "id": "H24v7knlprYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}